{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Han529/ML-Data-Science-Apps/blob/master/Junior_Data_Analytics_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2tx88Lr_mqw",
        "outputId": "2a0143aa-1e40-4c31-84f0-c3a8e5811e38"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            " Stage 1: Scraping Manager Links\n",
            "======================================================================\n",
            "Found 9 unique relative manager links.\n",
            "\n",
            "======================================================================\n",
            " Stage 2: Scraping Quarter Links for Each Manager\n",
            "======================================================================\n",
            "\n",
            "[1/9] Getting quarter links for Manager: /manager/0001536411-duquesne-family-office-llc\n",
            "\n",
            "[2/9] Getting quarter links for Manager: /manager/0001336528-pershing-square-capital-management-l-p\n",
            "\n",
            "[3/9] Getting quarter links for Manager: /manager/0001350694-bridgewater-associates-lp\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:'Quarter' column header not found in table on https://13f.info/manager/0001067983/cusip/037833100.\n",
            "WARNING:root:No quarter links found or error occurred for /manager/0001067983/cusip/037833100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[4/9] Getting quarter links for Manager: /manager/0001067983/cusip/037833100\n",
            "\n",
            "[5/9] Getting quarter links for Manager: /manager/0001067983-berkshire-hathaway-inc\n",
            "\n",
            "[6/9] Getting quarter links for Manager: /manager/0001364742-blackrock-inc\n",
            "\n",
            "[7/9] Getting quarter links for Manager: /manager/0001167483-tiger-global-management-llc\n",
            "\n",
            "[8/9] Getting quarter links for Manager: /manager/0001656456-appaloosa-lp\n",
            "\n",
            "[9/9] Getting quarter links for Manager: /manager/0001649339-scion-asset-management-llc\n",
            "\n",
            "Finished getting quarter links for 9 managers processed.\n",
            "\n",
            "======================================================================\n",
            " Stage 3: Scraping Table Data and Metadata for Each Filing\n",
            "======================================================================\n",
            "\n",
            ">>> Processing Manager 1/9: /manager/0001536411-duquesne-family-office-llc <<<\n",
            "    Fund Name: Duquesne Family Office LLC\n",
            "    Preparing to scrape data from 47 URL(s)...\n",
            "      -> Scraping URL [1/47]: https://13f.info/13f/000153641125000006-duquesne-family-office-llc-q4-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [2/47]: https://13f.info/13f/000153641124000009-duquesne-family-office-llc-q3-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [3/47]: https://13f.info/13f/000153641124000007-duquesne-family-office-llc-q2-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [4/47]: https://13f.info/13f/000153641124000005-duquesne-family-office-llc-q1-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [5/47]: https://13f.info/13f/000153641124000002-duquesne-family-office-llc-q4-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [6/47]: https://13f.info/13f/000153641123000008-duquesne-family-office-llc-q3-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [7/47]: https://13f.info/13f/000153641123000006-duquesne-family-office-llc-q2-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [8/47]: https://13f.info/13f/000153641123000005-duquesne-family-office-llc-q1-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [9/47]: https://13f.info/13f/000153641123000003-duquesne-family-office-llc-q4-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [10/47]: https://13f.info/13f/000153641122000009-duquesne-family-office-llc-q3-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [11/47]: https://13f.info/13f/000153641122000007-duquesne-family-office-llc-q2-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [12/47]: https://13f.info/13f/000153641122000005-duquesne-family-office-llc-q1-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [13/47]: https://13f.info/13f/000153641122000003-duquesne-family-office-llc-q4-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [14/47]: https://13f.info/13f/000153641121000008-duquesne-family-office-llc-q3-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [15/47]: https://13f.info/13f/000156761921020655-duquesne-family-office-llc-q2-2021-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [16/47]: https://13f.info/13f/000153641121000006-duquesne-family-office-llc-q2-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [17/47]: https://13f.info/13f/000156761921020654-duquesne-family-office-llc-q1-2021-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [18/47]: https://13f.info/13f/000153641121000004-duquesne-family-office-llc-q1-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [19/47]: https://13f.info/13f/000153641121000002-duquesne-family-office-llc-q4-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [20/47]: https://13f.info/13f/000153641120000008-duquesne-family-office-llc-q3-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [21/47]: https://13f.info/13f/000153641120000006-duquesne-family-office-llc-q2-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [22/47]: https://13f.info/13f/000153641120000004-duquesne-family-office-llc-q1-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [23/47]: https://13f.info/13f/000153641120000002-duquesne-family-office-llc-q4-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [24/47]: https://13f.info/13f/000153641119000009-duquesne-family-office-llc-q3-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [25/47]: https://13f.info/13f/000153641119000007-duquesne-family-office-llc-q2-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [26/47]: https://13f.info/13f/000153641119000005-duquesne-family-office-llc-q1-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [27/47]: https://13f.info/13f/000153641119000002-duquesne-family-office-llc-q4-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [28/47]: https://13f.info/13f/000153641118000014-duquesne-family-office-llc-q3-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [29/47]: https://13f.info/13f/000153641118000011-duquesne-family-office-llc-q2-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [30/47]: https://13f.info/13f/000153641118000008-duquesne-family-office-llc-q1-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [31/47]: https://13f.info/13f/000153641118000006-duquesne-family-office-llc-q4-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [32/47]: https://13f.info/13f/000153641117000011-duquesne-family-office-llc-q3-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [33/47]: https://13f.info/13f/000153641117000009-duquesne-family-office-llc-q2-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [34/47]: https://13f.info/13f/000153641117000007-duquesne-family-office-llc-q1-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [35/47]: https://13f.info/13f/000153641117000005-duquesne-family-office-llc-q4-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [36/47]: https://13f.info/13f/000153641116000019-duquesne-family-office-llc-q3-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [37/47]: https://13f.info/13f/000153641116000014-duquesne-family-office-llc-q2-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [38/47]: https://13f.info/13f/000153641116000012-duquesne-family-office-llc-q1-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [39/47]: https://13f.info/13f/000153641116000010-duquesne-family-office-llc-q4-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [40/47]: https://13f.info/13f/000153641115000008-duquesne-family-office-llc-q3-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [41/47]: https://13f.info/13f/000153641115000006-duquesne-family-office-llc-q2-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [42/47]: https://13f.info/13f/000153641115000004-duquesne-family-office-llc-q1-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [43/47]: https://13f.info/13f/000153641115000002-duquesne-family-office-llc-q4-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [44/47]: https://13f.info/13f/000153641114000009-duquesne-family-office-llc-q3-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [45/47]: https://13f.info/13f/000153641114000007-duquesne-family-office-llc-q2-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [46/47]: https://13f.info/13f/000153641114000005-duquesne-family-office-llc-q1-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [47/47]: https://13f.info/13f/000153641114000003-duquesne-family-office-llc-q4-2013\n",
            "        -- Found 1 table(s) from this URL --\n",
            "\n",
            ">>> Processing Manager 2/9: /manager/0001336528-pershing-square-capital-management-l-p <<<\n",
            "    Fund Name: Pershing Square Capital Management, L.P.\n",
            "    Preparing to scrape data from 48 URL(s)...\n",
            "      -> Scraping URL [1/48]: https://13f.info/13f/000117266125001497-pershing-square-capital-management-l-p-q4-2024-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [2/48]: https://13f.info/13f/000117266125001119-pershing-square-capital-management-l-p-q4-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [3/48]: https://13f.info/13f/000117266124005218-pershing-square-capital-management-l-p-q3-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [4/48]: https://13f.info/13f/000117266124003511-pershing-square-capital-management-l-p-q2-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [5/48]: https://13f.info/13f/000117266124002519-pershing-square-capital-management-l-p-q1-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [6/48]: https://13f.info/13f/000117266124001556-pershing-square-capital-management-l-p-q4-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [7/48]: https://13f.info/13f/000117266123004016-pershing-square-capital-management-l-p-q3-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [8/48]: https://13f.info/13f/000117266123003137-pershing-square-capital-management-l-p-q2-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [9/48]: https://13f.info/13f/000117266123002303-pershing-square-capital-management-l-p-q1-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [10/48]: https://13f.info/13f/000117266123000673-pershing-square-capital-management-l-p-q4-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [11/48]: https://13f.info/13f/000117266122002568-pershing-square-capital-management-l-p-q3-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [12/48]: https://13f.info/13f/000117266122002007-pershing-square-capital-management-l-p-q2-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [13/48]: https://13f.info/13f/000117266122001442-pershing-square-capital-management-l-p-q1-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [14/48]: https://13f.info/13f/000117266122001013-pershing-square-capital-management-l-p-q4-2021-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [15/48]: https://13f.info/13f/000117266122000776-pershing-square-capital-management-l-p-q4-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [16/48]: https://13f.info/13f/000117266121002355-pershing-square-capital-management-l-p-q3-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [17/48]: https://13f.info/13f/000117266121001865-pershing-square-capital-management-l-p-q2-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [18/48]: https://13f.info/13f/000117266121001367-pershing-square-capital-management-l-p-q1-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [19/48]: https://13f.info/13f/000117266121000814-pershing-square-capital-management-l-p-q4-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [20/48]: https://13f.info/13f/000117266120002031-pershing-square-capital-management-l-p-q3-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [21/48]: https://13f.info/13f/000117266120001845-pershing-square-capital-management-l-p-q2-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [22/48]: https://13f.info/13f/000117266120001409-pershing-square-capital-management-l-p-q1-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [23/48]: https://13f.info/13f/000117266120000821-pershing-square-capital-management-l-p-q4-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [24/48]: https://13f.info/13f/000117266119002404-pershing-square-capital-management-l-p-q3-2019-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [25/48]: https://13f.info/13f/000117266119002370-pershing-square-capital-management-l-p-q3-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [26/48]: https://13f.info/13f/000117266119001860-pershing-square-capital-management-l-p-q2-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [27/48]: https://13f.info/13f/000117266119001365-pershing-square-capital-management-l-p-q1-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [28/48]: https://13f.info/13f/000117266119000851-pershing-square-capital-management-l-p-q4-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [29/48]: https://13f.info/13f/000117266118002178-pershing-square-capital-management-l-p-q3-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [30/48]: https://13f.info/13f/000117266118001700-pershing-square-capital-management-l-p-q2-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [31/48]: https://13f.info/13f/000117266118001246-pershing-square-capital-management-l-p-q1-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [32/48]: https://13f.info/13f/000117266118000802-pershing-square-capital-management-l-p-q4-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [33/48]: https://13f.info/13f/000117266117002144-pershing-square-capital-management-l-p-q3-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [34/48]: https://13f.info/13f/000117266117001635-pershing-square-capital-management-l-p-q2-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [35/48]: https://13f.info/13f/000117266117001220-pershing-square-capital-management-l-p-q1-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [36/48]: https://13f.info/13f/000117266117000770-pershing-square-capital-management-l-p-q4-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [37/48]: https://13f.info/13f/000117266116004332-pershing-square-capital-management-l-p-q3-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [38/48]: https://13f.info/13f/000117266116003886-pershing-square-capital-management-l-p-q2-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [39/48]: https://13f.info/13f/000117266116003442-pershing-square-capital-management-l-p-q1-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [40/48]: https://13f.info/13f/000117266116002931-pershing-square-capital-management-l-p-q4-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [41/48]: https://13f.info/13f/000117266115002091-pershing-square-capital-management-l-p-q3-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [42/48]: https://13f.info/13f/000117266115001657-pershing-square-capital-management-l-p-q2-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [43/48]: https://13f.info/13f/000117266115001225-pershing-square-capital-management-l-p-q1-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [44/48]: https://13f.info/13f/000117266115000727-pershing-square-capital-management-l-p-q4-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [45/48]: https://13f.info/13f/000117266114001862-pershing-square-capital-management-l-p-q3-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [46/48]: https://13f.info/13f/000117266114001490-pershing-square-capital-management-l-p-q2-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [47/48]: https://13f.info/13f/000117266114001128-pershing-square-capital-management-l-p-q1-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [48/48]: https://13f.info/13f/000117266114000750-pershing-square-capital-management-l-p-q4-2013\n",
            "        -- Found 1 table(s) from this URL --\n",
            "\n",
            ">>> Processing Manager 3/9: /manager/0001350694-bridgewater-associates-lp <<<\n",
            "    Fund Name: Bridgewater Associates, LP\n",
            "    Preparing to scrape data from 47 URL(s)...\n",
            "      -> Scraping URL [1/47]: https://13f.info/13f/000117266125000823-bridgewater-associates-lp-q4-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [2/47]: https://13f.info/13f/000117266124004671-bridgewater-associates-lp-q3-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [3/47]: https://13f.info/13f/000117266124003581-bridgewater-associates-lp-q2-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [4/47]: https://13f.info/13f/000117266124002257-bridgewater-associates-lp-q1-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [5/47]: https://13f.info/13f/000117266124001126-bridgewater-associates-lp-q4-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [6/47]: https://13f.info/13f/000117266123003769-bridgewater-associates-lp-q3-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [7/47]: https://13f.info/13f/000117266123002943-bridgewater-associates-lp-q2-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [8/47]: https://13f.info/13f/000117266123002051-bridgewater-associates-lp-q1-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [9/47]: https://13f.info/13f/000117266123000737-bridgewater-associates-lp-q4-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [10/47]: https://13f.info/13f/000117266122002357-bridgewater-associates-lp-q3-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [11/47]: https://13f.info/13f/000117266122001788-bridgewater-associates-lp-q2-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [12/47]: https://13f.info/13f/000117266122001289-bridgewater-associates-lp-q1-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [13/47]: https://13f.info/13f/000117266122000562-bridgewater-associates-lp-q4-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [14/47]: https://13f.info/13f/000117266121002162-bridgewater-associates-lp-q3-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [15/47]: https://13f.info/13f/000117266121001668-bridgewater-associates-lp-q2-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [16/47]: https://13f.info/13f/000117266121001153-bridgewater-associates-lp-q1-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [17/47]: https://13f.info/13f/000156761921003290-bridgewater-associates-lp-q4-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [18/47]: https://13f.info/13f/000156761920019382-bridgewater-associates-lp-q3-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [19/47]: https://13f.info/13f/000156761920014895-bridgewater-associates-lp-q2-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [20/47]: https://13f.info/13f/000156761920010047-bridgewater-associates-lp-q1-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [21/47]: https://13f.info/13f/000156761920003264-bridgewater-associates-lp-q4-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [22/47]: https://13f.info/13f/000156761919021161-bridgewater-associates-lp-q3-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [23/47]: https://13f.info/13f/000156761919016421-bridgewater-associates-lp-q2-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [24/47]: https://13f.info/13f/000156761919010822-bridgewater-associates-lp-q1-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [25/47]: https://13f.info/13f/000156761919003499-bridgewater-associates-lp-q4-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [26/47]: https://13f.info/13f/000156761918005788-bridgewater-associates-lp-q3-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [27/47]: https://13f.info/13f/000156761918000648-bridgewater-associates-lp-q2-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [28/47]: https://13f.info/13f/000156761918000646-bridgewater-associates-lp-q1-2018-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [29/47]: https://13f.info/13f/000156761918000074-bridgewater-associates-lp-q1-2018-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [30/47]: https://13f.info/13f/000114036118023865-bridgewater-associates-lp-q1-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [31/47]: https://13f.info/13f/000114036118007141-bridgewater-associates-lp-q4-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [32/47]: https://13f.info/13f/000114036117041998-bridgewater-associates-lp-q3-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [33/47]: https://13f.info/13f/000114036117031111-bridgewater-associates-lp-q2-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [34/47]: https://13f.info/13f/000114036117019802-bridgewater-associates-lp-q1-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [35/47]: https://13f.info/13f/000114036117005803-bridgewater-associates-lp-q4-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [36/47]: https://13f.info/13f/000114036116085573-bridgewater-associates-lp-q3-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [37/47]: https://13f.info/13f/000114036116075565-bridgewater-associates-lp-q2-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [38/47]: https://13f.info/13f/000114036116065095-bridgewater-associates-lp-q1-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [39/47]: https://13f.info/13f/000114036116051801-bridgewater-associates-lp-q4-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [40/47]: https://13f.info/13f/000114036115040647-bridgewater-associates-lp-q3-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [41/47]: https://13f.info/13f/000114036115030974-bridgewater-associates-lp-q2-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [42/47]: https://13f.info/13f/000114036115019529-bridgewater-associates-lp-q1-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [43/47]: https://13f.info/13f/000114036115005578-bridgewater-associates-lp-q4-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [44/47]: https://13f.info/13f/000114036114041083-bridgewater-associates-lp-q3-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [45/47]: https://13f.info/13f/000114036114031924-bridgewater-associates-lp-q2-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [46/47]: https://13f.info/13f/000114036114021000-bridgewater-associates-lp-q1-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [47/47]: https://13f.info/13f/000114036114006358-bridgewater-associates-lp-q4-2013\n",
            "        -- Found 1 table(s) from this URL --\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:No specific quarter links found for /manager/0001067983/cusip/037833100, attempting scrape on manager page directly: https://13f.info/manager/0001067983/cusip/037833100\n",
            "WARNING:root:Target table (id='filingAggregated') not found. Parsing all tables on page: https://13f.info/manager/0001067983/cusip/037833100\n",
            "WARNING:root:[Direct BS4] Skipping table 1 on https://13f.info/manager/0001067983/cusip/037833100 - Headers found but no data rows (tbody/tr).\n",
            "WARNING:root:No data tables were successfully extracted from https://13f.info/manager/0001067983/cusip/037833100.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Processing Manager 4/9: /manager/0001067983/cusip/037833100 <<<\n",
            "    Fund Name: Berkshire Hathaway Inc\n",
            "    Preparing to scrape data from 1 URL(s)...\n",
            "      -> Scraping URL [1/1]: https://13f.info/manager/0001067983/cusip/037833100\n",
            "        -- No data tables extracted from this URL (check warnings/errors above) --\n",
            "\n",
            ">>> Processing Manager 5/9: /manager/0001067983-berkshire-hathaway-inc <<<\n",
            "    Fund Name: Berkshire Hathaway Inc\n",
            "    Preparing to scrape data from 50 URL(s)...\n",
            "      -> Scraping URL [1/50]: https://13f.info/13f/000095012325002701-berkshire-hathaway-inc-q4-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [2/50]: https://13f.info/13f/000095012324011775-berkshire-hathaway-inc-q3-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [3/50]: https://13f.info/13f/000095012324008740-berkshire-hathaway-inc-q2-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [4/50]: https://13f.info/13f/000095012324005622-berkshire-hathaway-inc-q1-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [5/50]: https://13f.info/13f/000095012324005664-berkshire-hathaway-inc-q4-2023-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [6/50]: https://13f.info/13f/000095012324002518-berkshire-hathaway-inc-q4-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [7/50]: https://13f.info/13f/000095012324005653-berkshire-hathaway-inc-q3-2023-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [8/50]: https://13f.info/13f/000095012323011029-berkshire-hathaway-inc-q3-2023-restatement\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [9/50]: https://13f.info/13f/000095012323008074-berkshire-hathaway-inc-q2-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [10/50]: https://13f.info/13f/000095012323005270-berkshire-hathaway-inc-q1-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [11/50]: https://13f.info/13f/000095012323002585-berkshire-hathaway-inc-q4-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [12/50]: https://13f.info/13f/000095012322012275-berkshire-hathaway-inc-q3-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [13/50]: https://13f.info/13f/000095012322009450-berkshire-hathaway-inc-q2-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [14/50]: https://13f.info/13f/000095012322006442-berkshire-hathaway-inc-q1-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [15/50]: https://13f.info/13f/000095012322002973-berkshire-hathaway-inc-q4-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [16/50]: https://13f.info/13f/000095012321015518-berkshire-hathaway-inc-q3-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [17/50]: https://13f.info/13f/000095012321011396-berkshire-hathaway-inc-q2-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [18/50]: https://13f.info/13f/000095012321007024-berkshire-hathaway-inc-q1-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [19/50]: https://13f.info/13f/000095012321002786-berkshire-hathaway-inc-q4-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [20/50]: https://13f.info/13f/000095012321002781-berkshire-hathaway-inc-q3-2020-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [21/50]: https://13f.info/13f/000095012320012127-berkshire-hathaway-inc-q3-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [22/50]: https://13f.info/13f/000095012320009058-berkshire-hathaway-inc-q2-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [23/50]: https://13f.info/13f/000095012320005345-berkshire-hathaway-inc-q1-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [24/50]: https://13f.info/13f/000095012320002466-berkshire-hathaway-inc-q4-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [25/50]: https://13f.info/13f/000095012319011362-berkshire-hathaway-inc-q3-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [26/50]: https://13f.info/13f/000095012319008356-berkshire-hathaway-inc-q2-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [27/50]: https://13f.info/13f/000095012319005436-berkshire-hathaway-inc-q1-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [28/50]: https://13f.info/13f/000095012319002221-berkshire-hathaway-inc-q4-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [29/50]: https://13f.info/13f/000095012318011885-berkshire-hathaway-inc-q3-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [30/50]: https://13f.info/13f/000095012318008866-berkshire-hathaway-inc-q2-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [31/50]: https://13f.info/13f/000095012318005732-berkshire-hathaway-inc-q1-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [32/50]: https://13f.info/13f/000095012318002390-berkshire-hathaway-inc-q4-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [33/50]: https://13f.info/13f/000095012317010896-berkshire-hathaway-inc-q3-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [34/50]: https://13f.info/13f/000095012317007953-berkshire-hathaway-inc-q2-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [35/50]: https://13f.info/13f/000095012317005259-berkshire-hathaway-inc-q1-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [36/50]: https://13f.info/13f/000095012317002417-berkshire-hathaway-inc-q4-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [37/50]: https://13f.info/13f/000095012316022377-berkshire-hathaway-inc-q3-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [38/50]: https://13f.info/13f/000095012316020120-berkshire-hathaway-inc-q2-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [39/50]: https://13f.info/13f/000095012316017295-berkshire-hathaway-inc-q1-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [40/50]: https://13f.info/13f/000095012316015025-berkshire-hathaway-inc-q4-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [41/50]: https://13f.info/13f/000095012315011992-berkshire-hathaway-inc-q3-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [42/50]: https://13f.info/13f/000095012315009742-berkshire-hathaway-inc-q2-2015-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [43/50]: https://13f.info/13f/000095012315009404-berkshire-hathaway-inc-q2-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [44/50]: https://13f.info/13f/000095012315006438-berkshire-hathaway-inc-q1-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [45/50]: https://13f.info/13f/000095012315002961-berkshire-hathaway-inc-q4-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [46/50]: https://13f.info/13f/000095012315002964-berkshire-hathaway-inc-q3-2014-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [47/50]: https://13f.info/13f/000095012314012218-berkshire-hathaway-inc-q3-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [48/50]: https://13f.info/13f/000095012314009048-berkshire-hathaway-inc-q2-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [49/50]: https://13f.info/13f/000095012314006252-berkshire-hathaway-inc-q1-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [50/50]: https://13f.info/13f/000095012314002615-berkshire-hathaway-inc-q4-2013\n",
            "        -- Found 1 table(s) from this URL --\n",
            "\n",
            ">>> Processing Manager 6/9: /manager/0001364742-blackrock-inc <<<\n",
            "    Fund Name: BlackRock Inc.\n",
            "    Preparing to scrape data from 46 URL(s)...\n",
            "      -> Scraping URL [1/46]: https://13f.info/13f/000108636424008417-blackrock-inc-q2-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [2/46]: https://13f.info/13f/000108636424007638-blackrock-inc-q1-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [3/46]: https://13f.info/13f/000108636424006999-blackrock-inc-q4-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [4/46]: https://13f.info/13f/000130655023010335-blackrock-inc-q3-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [5/46]: https://13f.info/13f/000130655023009732-blackrock-inc-q2-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [6/46]: https://13f.info/13f/000108636423000015-blackrock-inc-q1-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [7/46]: https://13f.info/13f/000091341423000035-blackrock-inc-q4-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [8/46]: https://13f.info/13f/000108636422000004-blackrock-inc-q3-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [9/46]: https://13f.info/13f/000083423722010526-blackrock-inc-q2-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [10/46]: https://13f.info/13f/000083423722009651-blackrock-inc-q1-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [11/46]: https://13f.info/13f/000083423722008704-blackrock-inc-q4-2021-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [12/46]: https://13f.info/13f/000083423722008654-blackrock-inc-q4-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [13/46]: https://13f.info/13f/000108636421000104-blackrock-inc-q3-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [14/46]: https://13f.info/13f/000108636421000071-blackrock-inc-q2-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [15/46]: https://13f.info/13f/000108636421000038-blackrock-inc-q1-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [16/46]: https://13f.info/13f/000108636421000004-blackrock-inc-q4-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [17/46]: https://13f.info/13f/000142645920000005-blackrock-inc-q3-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [18/46]: https://13f.info/13f/000108636420000070-blackrock-inc-q2-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [19/46]: https://13f.info/13f/000108636420000038-blackrock-inc-q1-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [20/46]: https://13f.info/13f/000108636420000006-blackrock-inc-q4-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [21/46]: https://13f.info/13f/000108636419000103-blackrock-inc-q3-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [22/46]: https://13f.info/13f/000108636419000100-blackrock-inc-q2-2019-restatement\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [23/46]: https://13f.info/13f/000108636419000034-blackrock-inc-q1-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [24/46]: https://13f.info/13f/000108636419000003-blackrock-inc-q4-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [25/46]: https://13f.info/13f/000108636418000095-blackrock-inc-q3-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [26/46]: https://13f.info/13f/000108636418000066-blackrock-inc-q2-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [27/46]: https://13f.info/13f/000108636418000035-blackrock-inc-q1-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [28/46]: https://13f.info/13f/000108636418000003-blackrock-inc-q4-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [29/46]: https://13f.info/13f/000108636417000128-blackrock-inc-q3-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [30/46]: https://13f.info/13f/000108636417000071-blackrock-inc-q2-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [31/46]: https://13f.info/13f/000108636417000069-blackrock-inc-q1-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [32/46]: https://13f.info/13f/000108636417000005-blackrock-inc-q4-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [33/46]: https://13f.info/13f/000108636416002326-blackrock-inc-q3-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [34/46]: https://13f.info/13f/000103455116000004-blackrock-inc-q2-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [35/46]: https://13f.info/13f/000108636416002309-blackrock-inc-q1-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [36/46]: https://13f.info/13f/000108636416002274-blackrock-inc-q4-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [37/46]: https://13f.info/13f/000108636415002209-blackrock-inc-q3-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [38/46]: https://13f.info/13f/000108636415002186-blackrock-inc-q2-2015-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [39/46]: https://13f.info/13f/000108636415002170-blackrock-inc-q2-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [40/46]: https://13f.info/13f/000108636415002125-blackrock-inc-q1-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [41/46]: https://13f.info/13f/000108636415002009-blackrock-inc-q4-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [42/46]: https://13f.info/13f/000108636414002622-blackrock-inc-q3-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [43/46]: https://13f.info/13f/000108636414002503-blackrock-inc-q2-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [44/46]: https://13f.info/13f/000108636414002322-blackrock-inc-q1-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [45/46]: https://13f.info/13f/000108636414002248-blackrock-inc-q4-2013-new-holdings\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [46/46]: https://13f.info/13f/000108636414002230-blackrock-inc-q4-2013\n",
            "        -- Found 1 table(s) from this URL --\n",
            "\n",
            ">>> Processing Manager 7/9: /manager/0001167483-tiger-global-management-llc <<<\n",
            "    Fund Name: TIGER GLOBAL MANAGEMENT LLC\n",
            "    Preparing to scrape data from 45 URL(s)...\n",
            "      -> Scraping URL [1/45]: https://13f.info/13f/000091957425001475-tiger-global-management-llc-q4-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [2/45]: https://13f.info/13f/000091957424006690-tiger-global-management-llc-q3-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [3/45]: https://13f.info/13f/000091957424004713-tiger-global-management-llc-q2-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [4/45]: https://13f.info/13f/000091957424003172-tiger-global-management-llc-q1-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [5/45]: https://13f.info/13f/000091957424001349-tiger-global-management-llc-q4-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [6/45]: https://13f.info/13f/000091957423006372-tiger-global-management-llc-q3-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [7/45]: https://13f.info/13f/000091957423004771-tiger-global-management-llc-q2-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [8/45]: https://13f.info/13f/000091957423003305-tiger-global-management-llc-q1-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [9/45]: https://13f.info/13f/000091957423001481-tiger-global-management-llc-q4-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [10/45]: https://13f.info/13f/000091957422006727-tiger-global-management-llc-q3-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [11/45]: https://13f.info/13f/000091957422005053-tiger-global-management-llc-q2-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [12/45]: https://13f.info/13f/000091957422003470-tiger-global-management-llc-q1-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [13/45]: https://13f.info/13f/000091957422001371-tiger-global-management-llc-q4-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [14/45]: https://13f.info/13f/000091957421007099-tiger-global-management-llc-q3-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [15/45]: https://13f.info/13f/000091957421005372-tiger-global-management-llc-q2-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [16/45]: https://13f.info/13f/000091957421003697-tiger-global-management-llc-q1-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [17/45]: https://13f.info/13f/000091957421001376-tiger-global-management-llc-q4-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [18/45]: https://13f.info/13f/000091957420007148-tiger-global-management-llc-q3-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [19/45]: https://13f.info/13f/000091957420005337-tiger-global-management-llc-q2-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [20/45]: https://13f.info/13f/000091957420003646-tiger-global-management-llc-q1-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [21/45]: https://13f.info/13f/000091957420001541-tiger-global-management-llc-q4-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [22/45]: https://13f.info/13f/000091957419007198-tiger-global-management-llc-q3-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [23/45]: https://13f.info/13f/000091957419005381-tiger-global-management-llc-q2-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [24/45]: https://13f.info/13f/000091957419003690-tiger-global-management-llc-q1-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [25/45]: https://13f.info/13f/000091957419001612-tiger-global-management-llc-q4-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [26/45]: https://13f.info/13f/000091957418007445-tiger-global-management-llc-q3-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [27/45]: https://13f.info/13f/000091957418005626-tiger-global-management-llc-q2-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [28/45]: https://13f.info/13f/000091957418003754-tiger-global-management-llc-q1-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [29/45]: https://13f.info/13f/000091957418001706-tiger-global-management-llc-q4-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [30/45]: https://13f.info/13f/000091957417008069-tiger-global-management-llc-q3-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [31/45]: https://13f.info/13f/000091957417006218-tiger-global-management-llc-q2-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [32/45]: https://13f.info/13f/000091957417004286-tiger-global-management-llc-q1-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [33/45]: https://13f.info/13f/000091957417001974-tiger-global-management-llc-q4-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [34/45]: https://13f.info/13f/000091957416016596-tiger-global-management-llc-q3-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [35/45]: https://13f.info/13f/000091957416014985-tiger-global-management-llc-q2-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [36/45]: https://13f.info/13f/000091957416013223-tiger-global-management-llc-q1-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [37/45]: https://13f.info/13f/000091957416011040-tiger-global-management-llc-q4-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [38/45]: https://13f.info/13f/000091957415008225-tiger-global-management-llc-q3-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [39/45]: https://13f.info/13f/000091957415006282-tiger-global-management-llc-q2-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [40/45]: https://13f.info/13f/000091957415004426-tiger-global-management-llc-q1-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [41/45]: https://13f.info/13f/000091957415002170-tiger-global-management-llc-q4-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [42/45]: https://13f.info/13f/000091957414006596-tiger-global-management-llc-q3-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [43/45]: https://13f.info/13f/000091957414004747-tiger-global-management-llc-q2-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [44/45]: https://13f.info/13f/000091957414003349-tiger-global-management-llc-q1-2014\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [45/45]: https://13f.info/13f/000091957414001717-tiger-global-management-llc-q4-2013\n",
            "        -- Found 1 table(s) from this URL --\n",
            "\n",
            ">>> Processing Manager 8/9: /manager/0001656456-appaloosa-lp <<<\n",
            "    Fund Name: APPALOOSA LP\n",
            "    Preparing to scrape data from 36 URL(s)...\n",
            "      -> Scraping URL [1/36]: https://13f.info/13f/000165645625000001-appaloosa-lp-q4-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [2/36]: https://13f.info/13f/000165645624000004-appaloosa-lp-q3-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [3/36]: https://13f.info/13f/000165645624000003-appaloosa-lp-q2-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [4/36]: https://13f.info/13f/000165645624000002-appaloosa-lp-q1-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [5/36]: https://13f.info/13f/000165645624000001-appaloosa-lp-q4-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [6/36]: https://13f.info/13f/000165645623000004-appaloosa-lp-q3-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [7/36]: https://13f.info/13f/000165645623000003-appaloosa-lp-q2-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [8/36]: https://13f.info/13f/000165645623000002-appaloosa-lp-q1-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [9/36]: https://13f.info/13f/000165645623000001-appaloosa-lp-q4-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [10/36]: https://13f.info/13f/000165645622000004-appaloosa-lp-q3-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [11/36]: https://13f.info/13f/000165645622000003-appaloosa-lp-q2-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [12/36]: https://13f.info/13f/000165645622000002-appaloosa-lp-q1-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [13/36]: https://13f.info/13f/000165645622000001-appaloosa-lp-q4-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [14/36]: https://13f.info/13f/000165645621000004-appaloosa-lp-q3-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [15/36]: https://13f.info/13f/000165645621000003-appaloosa-lp-q2-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [16/36]: https://13f.info/13f/000165645621000002-appaloosa-lp-q1-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [17/36]: https://13f.info/13f/000165645621000001-appaloosa-lp-q4-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [18/36]: https://13f.info/13f/000165645620000004-appaloosa-lp-q3-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [19/36]: https://13f.info/13f/000165645620000003-appaloosa-lp-q2-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [20/36]: https://13f.info/13f/000165645620000002-appaloosa-lp-q1-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [21/36]: https://13f.info/13f/000165645620000001-appaloosa-lp-q4-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [22/36]: https://13f.info/13f/000165645619000008-appaloosa-lp-q3-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [23/36]: https://13f.info/13f/000165645619000006-appaloosa-lp-q2-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [24/36]: https://13f.info/13f/000165645619000005-appaloosa-lp-q1-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [25/36]: https://13f.info/13f/000165645619000004-appaloosa-lp-q4-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [26/36]: https://13f.info/13f/000165645618000007-appaloosa-lp-q3-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [27/36]: https://13f.info/13f/000165645618000006-appaloosa-lp-q2-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [28/36]: https://13f.info/13f/000165645618000004-appaloosa-lp-q1-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [29/36]: https://13f.info/13f/000165645618000003-appaloosa-lp-q4-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [30/36]: https://13f.info/13f/000165645617000005-appaloosa-lp-q3-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [31/36]: https://13f.info/13f/000165645617000004-appaloosa-lp-q2-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [32/36]: https://13f.info/13f/000165645617000003-appaloosa-lp-q1-2017\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [33/36]: https://13f.info/13f/000165645617000002-appaloosa-lp-q4-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [34/36]: https://13f.info/13f/000165645616000010-appaloosa-lp-q3-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [35/36]: https://13f.info/13f/000165645616000008-appaloosa-lp-q2-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [36/36]: https://13f.info/13f/000165645616000007-appaloosa-lp-q1-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "\n",
            ">>> Processing Manager 9/9: /manager/0001649339-scion-asset-management-llc <<<\n",
            "    Fund Name: Scion Asset Management, LLC\n",
            "    Preparing to scrape data from 29 URL(s)...\n",
            "      -> Scraping URL [1/29]: https://13f.info/13f/000187920225000012-scion-asset-management-llc-q4-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [2/29]: https://13f.info/13f/000090514824003106-scion-asset-management-llc-q3-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [3/29]: https://13f.info/13f/000090514824002196-scion-asset-management-llc-q2-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [4/29]: https://13f.info/13f/000090514824001390-scion-asset-management-llc-q1-2024\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [5/29]: https://13f.info/13f/000090514824000751-scion-asset-management-llc-q4-2023-restatement\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [6/29]: https://13f.info/13f/000090514823001400-scion-asset-management-llc-q3-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [7/29]: https://13f.info/13f/000090514823000689-scion-asset-management-llc-q2-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [8/29]: https://13f.info/13f/000090514823000408-scion-asset-management-llc-q1-2023\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [9/29]: https://13f.info/13f/000090514823000213-scion-asset-management-llc-q4-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [10/29]: https://13f.info/13f/000156761922019784-scion-asset-management-llc-q3-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [11/29]: https://13f.info/13f/000156761922015999-scion-asset-management-llc-q2-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [12/29]: https://13f.info/13f/000156761922010747-scion-asset-management-llc-q1-2022\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [13/29]: https://13f.info/13f/000156761922003986-scion-asset-management-llc-q4-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [14/29]: https://13f.info/13f/000156761921020215-scion-asset-management-llc-q3-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [15/29]: https://13f.info/13f/000156761921015632-scion-asset-management-llc-q2-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [16/29]: https://13f.info/13f/000156761921010281-scion-asset-management-llc-q1-2021\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [17/29]: https://13f.info/13f/000156761921003819-scion-asset-management-llc-q4-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [18/29]: https://13f.info/13f/000156761920019679-scion-asset-management-llc-q3-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [19/29]: https://13f.info/13f/000156761920015233-scion-asset-management-llc-q2-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [20/29]: https://13f.info/13f/000156761920010271-scion-asset-management-llc-q1-2020\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [21/29]: https://13f.info/13f/000156761920003530-scion-asset-management-llc-q4-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [22/29]: https://13f.info/13f/000156761919021303-scion-asset-management-llc-q3-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [23/29]: https://13f.info/13f/000156761919016689-scion-asset-management-llc-q2-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [24/29]: https://13f.info/13f/000156761919010955-scion-asset-management-llc-q1-2019\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [25/29]: https://13f.info/13f/000156761919004198-scion-asset-management-llc-q4-2018\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [26/29]: https://13f.info/13f/000114036116086026-scion-asset-management-llc-q3-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [27/29]: https://13f.info/13f/000114036116076357-scion-asset-management-llc-q2-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [28/29]: https://13f.info/13f/000114036116065328-scion-asset-management-llc-q1-2016\n",
            "        -- Found 1 table(s) from this URL --\n",
            "      -> Scraping URL [29/29]: https://13f.info/13f/000114036116052634-scion-asset-management-llc-q4-2015\n",
            "        -- Found 1 table(s) from this URL --\n",
            "\n",
            "--- Finished Scraping All Filing Data ---\n",
            "\n",
            "======================================================================\n",
            " Stage 4: Consolidating and Processing All Scraped Data\n",
            "======================================================================\n",
            "Combining 348 scraped tables...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-79a371a4c81f>:587: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_df = pd.concat(all_scraped_dataframes, ignore_index=True, sort=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined DataFrame shape: (233917, 13).\n",
            "\n",
            "Processing combined data...\n",
            "Filtered for 'COM' stock. Rows remaining: 141438.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-79a371a4c81f>:620: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  processed_df['shares'] = processed_df['shares'].astype(str).str.replace(',', '', regex=False)\n",
            "WARNING:root:Dropped 3037 rows with NaN in essential columns (['fund_name', 'sym', 'quarter_period', 'shares_numeric']) before change calculation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating quarterly changes grouped by fund and 'sym'...\n",
            "Quarterly change calculations complete.\n",
            "\n",
            "Formatting final columns for output...\n",
            "Final DataFrame columns: ['fund_name', 'filing_date', 'quarter', 'stock symbol', 'cl', 'value_usd_000', 'shares', 'change', 'pct_change', 'inferred_transaction_type']\n",
            "\n",
            "======================================================================\n",
            " Stage 5: Final Output\n",
            "======================================================================\n",
            "\n",
            "--- First 5 rows of the final DataFrame (Shape: (138401, 10)) ---\n",
            "           fund_name filing_date               quarter stock symbol   cl  value_usd_000     shares   change  pct_change inferred_transaction_type\n",
            "233385  APPALOOSA LP  11/14/2016  Q3 2016 13F Holdings         AABA  COM          77580  1800000.0        0    0.000000                      Hold\n",
            "233343  APPALOOSA LP   2/14/2017  Q4 2016 13F Holdings         AABA  COM          52205  1350000.0  -450000   -0.250000                      Sell\n",
            "233281  APPALOOSA LP   5/12/2017  Q1 2017 13F Holdings         AABA  COM         102102  2200000.0   850000    0.629630                       Buy\n",
            "233209  APPALOOSA LP   8/14/2017  Q2 2017 13F Holdings         AABA  COM         279431  5129051.0  2929051    1.331387                       Buy\n",
            "233159  APPALOOSA LP  11/14/2017  Q3 2017 13F Holdings         AABA  COM         310169  4682500.0  -446551   -0.087063                      Sell\n",
            "\n",
            "--- Saving final DataFrame to 'Fund Manager Shares Analysis.csv' ---\n",
            "Successfully saved data (138401 rows) to 'Fund Manager Shares Analysis.csv'\n",
            "\n",
            "--- Script execution finished ---\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "#                            Imports and Setup\n",
        "# ==============================================================================\n",
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "from urllib.parse import urljoin\n",
        "import json\n",
        "import logging\n",
        "from typing import List, Dict, Optional, Tuple, Any # For type hints\n",
        "\n",
        "# --- Configuration ---\n",
        "\n",
        "# Logging Setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)-8s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "# Suppress overly verbose logs from underlying libraries like urllib3\n",
        "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"requests\").setLevel(logging.WARNING)\n",
        "\n",
        "# --- Constants ---\n",
        "BASE_URL: str = \"https://13f.info\"\n",
        "DEFAULT_USER_AGENT: str = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "CSV_FILENAME: str = \"Fund Manager Shares Analysis.csv\"\n",
        "\n",
        "# Define the exact columns desired in the final output, in order\n",
        "TARGET_FINAL_COLUMNS: List[str] = [\n",
        "    'fund_name',\n",
        "    'filing_date',\n",
        "    'quarter',\n",
        "    'stock symbol', # Note: Renamed from 'sym' or 'stock_symbol'\n",
        "    'cl',\n",
        "    'value_usd_000', # Note: Assumes header cleaning produces this\n",
        "    'shares',\n",
        "    'change',\n",
        "    'pct_change',\n",
        "    'inferred_transaction_type'\n",
        "]\n",
        "\n",
        "logging.info(\"Script started. Libraries imported and logging configured.\")\n",
        "\n",
        "# ==============================================================================\n",
        "#                        Scraping Function Definitions\n",
        "# ==============================================================================\n",
        "\n",
        "# ***** CORRECTED scrape_manager_links FUNCTION *****\n",
        "def scrape_manager_links(url: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Scrapes relative links for individual managers from the main 13f.info page.\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the page listing managers (e.g., BASE_URL + \"/\").\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of unique relative URL paths (e.g., '/manager/...')\n",
        "                   or an empty list if scraping fails.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Scraping manager links from: {url}\")\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    try:\n",
        "        # Fetch the page content\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status() # Check for HTTP errors (4xx, 5xx)\n",
        "\n",
        "        # Parse the HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = soup.find_all('a', href=True)\n",
        "\n",
        "        # Filter for relative links starting with '/manager/'\n",
        "        # Ensure href exists and is a string before checking startswith\n",
        "        # Use a set for automatic uniqueness, then convert back to list\n",
        "        manager_links_relative = list(set(\n",
        "            link['href'] for link in links\n",
        "            if isinstance(link.get('href'), str) and link['href'].startswith('/manager/')\n",
        "        ))\n",
        "\n",
        "        logging.info(f\"Found {len(manager_links_relative)} unique relative manager links.\")\n",
        "        return manager_links_relative\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        # Handle network/HTTP errors during the request\n",
        "        logging.error(f\"Network or HTTP error scraping manager links from {url}. Error: {e}\")\n",
        "        return [] # Return empty list on failure\n",
        "    except Exception as e:\n",
        "        # Handle any other unexpected errors (e.g., during parsing)\n",
        "        logging.error(f\"Unexpected error scraping manager links from page {url}. Error: {e}\")\n",
        "        return [] # Return empty list on failure\n",
        "# ***** END OF CORRECTED scrape_manager_links FUNCTION *****\n",
        "\n",
        "# ---\n",
        "\n",
        "def scrape_quarter_links_from_manager(relative_manager_link: str, base_url: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Scrapes the absolute URLs for quarterly filings from a specific manager's page.\n",
        "    Looks for links within the 'Quarter' column of the main table.\n",
        "\n",
        "    Args:\n",
        "        relative_manager_link (str): The relative path of the manager's page.\n",
        "        base_url (str): The base URL of the site (e.g., \"https://13f.info\").\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of absolute URLs for quarterly filings, or empty list on failure.\n",
        "    \"\"\"\n",
        "    full_url = urljoin(base_url, relative_manager_link)\n",
        "    logging.info(f\"Scraping quarter links from: {full_url}\")\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    quarter_links = []\n",
        "    try:\n",
        "        response = requests.get(full_url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        # Find the primary data table (more flexible selector)\n",
        "        table = soup.find('table', {'class': re.compile(r'\\btable\\b')}) # Find any table with 'table' class\n",
        "        if not table:\n",
        "            logging.warning(f\"No data table found on manager page {full_url}.\")\n",
        "            return []\n",
        "\n",
        "        # Find 'Quarter' column index robustly\n",
        "        header_row = table.find('thead') or table.find('tr')\n",
        "        if not header_row:\n",
        "            logging.warning(f\"No table header found on {full_url}.\")\n",
        "            return []\n",
        "        headers_list = header_row.find_all(['th', 'td'])\n",
        "        quarter_col_index = -1\n",
        "        for i, header in enumerate(headers_list):\n",
        "            # Use lower() for case-insensitive comparison\n",
        "            if header.get_text(strip=True).lower() == \"quarter\":\n",
        "                quarter_col_index = i\n",
        "                break\n",
        "        if quarter_col_index == -1:\n",
        "            logging.warning(f\"'Quarter' column header not found in table on {full_url}.\")\n",
        "            return []\n",
        "\n",
        "        # Find data rows (tbody or trs after header)\n",
        "        tbody = table.find('tbody')\n",
        "        data_rows = tbody.find_all('tr') if tbody else (table.find_all('tr')[1:] if len(table.find_all('tr')) > 1 else [])\n",
        "        if not data_rows:\n",
        "            logging.warning(f\"No data rows found in table on {full_url}.\")\n",
        "            return []\n",
        "\n",
        "        # Extract links from the correct column\n",
        "        for row in data_rows:\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) > quarter_col_index:\n",
        "                link_tag = cells[quarter_col_index].find('a', href=True)\n",
        "                # Check if href exists and is a non-empty string\n",
        "                href_val = link_tag.get('href') if link_tag else None\n",
        "                if isinstance(href_val, str) and href_val.strip():\n",
        "                    quarter_links.append(urljoin(base_url, href_val)) # Ensure absolute URL\n",
        "\n",
        "        logging.info(f\"Found {len(quarter_links)} quarter links on {full_url}.\")\n",
        "        return quarter_links\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Network/HTTP error retrieving manager page {full_url} for quarter links. Error: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unexpected error parsing manager page {full_url} for quarter links. Error: {e}\")\n",
        "        return []\n",
        "\n",
        "# ---\n",
        "\n",
        "def scrape_fund_name(manager_page_url: str) -> str:\n",
        "    \"\"\"\n",
        "    Scrapes the fund name from the manager's main page (typically H1 tag).\n",
        "    Includes fallback logic to parse name from URL path if H1 is missing.\n",
        "\n",
        "    Args:\n",
        "        manager_page_url (str): The absolute URL of the manager's overview page.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted fund name, or \"Unknown Fund Name\" if extraction fails.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Scraping fund name from: {manager_page_url}\")\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT}\n",
        "    default_name = \"Unknown Fund Name\"\n",
        "    try:\n",
        "        response = requests.get(manager_page_url, headers=headers, timeout=25)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'lxml') # Use lxml for parsing efficiency\n",
        "        h1_tag = soup.find('h1')\n",
        "        if h1_tag:\n",
        "            name = h1_tag.get_text(strip=True)\n",
        "            # Basic cleaning to remove common suffixes like CIK number\n",
        "            if ' CIK#' in name: name = name.split(' CIK#')[0].strip()\n",
        "            logging.info(f\"Found fund name via H1: {name}\")\n",
        "            return name if name else default_name\n",
        "        else: # Fallback to URL parsing\n",
        "            logging.warning(f\"H1 tag not found on {manager_page_url}. Attempting URL parsing for name.\")\n",
        "            try:\n",
        "                path_parts = manager_page_url.strip('/').split('/')\n",
        "                # Expecting format like ['https:', '', '13f.info', 'manager', '000...-fund-name-inc']\n",
        "                if len(path_parts) > 3 and path_parts[-2] == 'manager':\n",
        "                    url_name_part = path_parts[-1]\n",
        "                    # Find first dash to separate ID from name part\n",
        "                    first_dash_index = url_name_part.find('-')\n",
        "                    if first_dash_index != -1:\n",
        "                         # Take part after first dash, remove leading/trailing dashes, title case\n",
        "                         parsed_name = url_name_part[first_dash_index:].strip('-').replace('-', ' ').title()\n",
        "                         if parsed_name:\n",
        "                            logging.info(f\"Using parsed name from URL: {parsed_name}\")\n",
        "                            return parsed_name\n",
        "            except Exception as url_parse_e: # Catch potential errors during list indexing/splitting\n",
        "                 logging.warning(f\"URL parsing for name failed: {url_parse_e}\")\n",
        "            logging.warning(f\"Could not determine fund name for {manager_page_url}.\")\n",
        "            return default_name\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Network/HTTP error retrieving {manager_page_url} for name. Error: {e}\")\n",
        "        return default_name\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unexpected error parsing {manager_page_url} for name: {e}\")\n",
        "        return default_name\n",
        "\n",
        "# ---\n",
        "\n",
        "def _clean_header(header_text: str) -> str:\n",
        "    \"\"\"Helper function to consistently clean table header text.\"\"\"\n",
        "    if not isinstance(header_text, str): return \"\"\n",
        "    text = header_text.lower().strip()\n",
        "    text = text.replace(' ', '_')\n",
        "    text = text.replace('($000)', 'usd_000') # Specific replacement for value column\n",
        "    text = text.replace('%', 'pct')\n",
        "    # Remove any remaining non-alphanumeric characters except underscore\n",
        "    text = re.sub(r'[^\\w_]+', '', text)\n",
        "    return text\n",
        "\n",
        "# ---\n",
        "\n",
        "def _extract_headers(table_tag: BeautifulSoup) -> List[str]:\n",
        "    \"\"\"Robustly extracts and cleans header text from a table tag.\"\"\"\n",
        "    header_cells = []\n",
        "    # Strategy 1: thead > tr > th/td\n",
        "    thead = table_tag.find('thead')\n",
        "    if thead:\n",
        "        header_row = thead.find('tr')\n",
        "        if header_row: header_cells = header_row.find_all(['th', 'td'])\n",
        "        # Strategy 2: thead > th (less common)\n",
        "        if not header_cells: header_cells = thead.find_all('th')\n",
        "    # Strategy 3: Fallback to first tr in table > th/td\n",
        "    if not header_cells:\n",
        "        first_row = table_tag.find('tr')\n",
        "        if first_row: header_cells = first_row.find_all(['th', 'td'])\n",
        "\n",
        "    if not header_cells:\n",
        "        logging.warning(\"Could not find any header cells (th/td) using multiple strategies.\")\n",
        "        return []\n",
        "\n",
        "    headers = [_clean_header(h.get_text()) for h in header_cells]\n",
        "    # Filter out empty headers that might result from cleaning\n",
        "    headers = [h for h in headers if h]\n",
        "    logging.info(f\"Extracted {len(headers)} headers: {headers}\")\n",
        "    return headers\n",
        "\n",
        "# --- Helper function for direct BS4 parsing (used as fallback) ---\n",
        "def parse_tables_directly_bs4(soup_or_table_tag: Any, url: str, single_table: bool = False) -> List[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Parses table(s) directly from HTML using BeautifulSoup. Used as a fallback.\n",
        "    Includes robust header finding and column mismatch handling.\n",
        "\n",
        "    Args:\n",
        "        soup_or_table_tag: BeautifulSoup soup object or a specific table Tag.\n",
        "        url (str): The source URL (for logging and SourceURL column).\n",
        "        single_table (bool): If True, treats input as a single table Tag.\n",
        "\n",
        "    Returns:\n",
        "        List[pd.DataFrame]: A list of DataFrames created from the parsed tables.\n",
        "    \"\"\"\n",
        "    dataframes = []\n",
        "    potential_tables = []\n",
        "    if single_table:\n",
        "        potential_tables = [soup_or_table_tag] if soup_or_table_tag else []\n",
        "        mode = \"single table\"\n",
        "    else:\n",
        "        # More flexible selector for finding tables\n",
        "        potential_tables = soup_or_table_tag.find_all('table', {'class': re.compile(r'\\btable\\b')}) if soup_or_table_tag else []\n",
        "        mode = \"all tables\"\n",
        "    logging.info(f\"[Direct BS4 - {mode}] Parsing {len(potential_tables)} potential table(s) on {url}\")\n",
        "\n",
        "    if not potential_tables:\n",
        "        logging.warning(f\"[Direct BS4 - {mode}] No tables found to parse on {url}\")\n",
        "        return []\n",
        "\n",
        "    for i, table_tag in enumerate(potential_tables):\n",
        "        logging.debug(f\"[Direct BS4] Processing table {i+1}\")\n",
        "        headers_list = _extract_headers(table_tag) # Use robust header extraction\n",
        "        if not headers_list:\n",
        "            logging.warning(f\"[Direct BS4] Skipping table {i+1} on {url} - No valid headers extracted.\")\n",
        "            continue\n",
        "\n",
        "        tbody = table_tag.find('tbody')\n",
        "        rows_in_body = tbody.find_all('tr') if tbody else (table_tag.find_all('tr')[1:] if len(table_tag.find_all('tr')) > 1 else [])\n",
        "        if not rows_in_body:\n",
        "             logging.warning(f\"[Direct BS4] Skipping table {i+1} on {url} - Headers found but no data rows (tbody/tr).\")\n",
        "             continue\n",
        "\n",
        "        data_rows = []\n",
        "        expected_cols = len(headers_list)\n",
        "        column_mismatch_logged = False\n",
        "        for row_idx, row in enumerate(rows_in_body):\n",
        "            cells = row.find_all('td')\n",
        "            actual_cols = len(cells)\n",
        "            if actual_cols == expected_cols:\n",
        "                # Extract text, handle potential None values gracefully\n",
        "                row_data = [c.get_text(strip=True) if c else None for c in cells]\n",
        "                data_rows.append(row_data)\n",
        "            else:\n",
        "                 # Log only once per table for mismatches\n",
        "                 if not column_mismatch_logged:\n",
        "                    logging.warning(f\"[Direct BS4] Table {i+1}: Row {row_idx} has {actual_cols} cells, expected {expected_cols}. Skipping mismatched rows for this table on {url}.\")\n",
        "                    column_mismatch_logged = True\n",
        "\n",
        "        if data_rows:\n",
        "            try:\n",
        "                df = pd.DataFrame(data_rows, columns=headers_list)\n",
        "                df['SourceURL'] = url\n",
        "                # Replace purely whitespace cells with NaN for better processing later\n",
        "                df = df.replace(r'^\\s*$', np.nan, regex=True)\n",
        "                dataframes.append(df)\n",
        "                logging.info(f\"[Direct BS4] Successfully created DataFrame (Shape: {df.shape}) from table {i+1} on {url}\")\n",
        "            except Exception as e:\n",
        "                 # Catch potential errors during DataFrame creation (e.g., duplicate columns)\n",
        "                 logging.error(f\"[Direct BS4] Error creating DataFrame for table {i+1} on {url}: {e}\", exc_info=True)\n",
        "        else:\n",
        "            logging.warning(f\"[Direct BS4] Extracted headers but no valid data rows constructed for table {i+1} on {url}\")\n",
        "    return dataframes\n",
        "\n",
        "# --- Main Data Scraping Function ---\n",
        "def scrape_table_data_and_metadata(url: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Scrapes table data, filing date, and quarter title from a given URL.\n",
        "    Prioritizes AJAX data loading if table has 'data-url' attribute.\n",
        "    Falls back to direct HTML parsing if AJAX fails or is not applicable.\n",
        "\n",
        "    Args:\n",
        "        url (str): The absolute URL of the quarterly filing page.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing:\n",
        "            'dataframes': List[pd.DataFrame] - List of extracted data tables.\n",
        "            'filing_date': Optional[str] - The extracted filing date.\n",
        "            'quarter': Optional[str] - The extracted quarter title.\n",
        "    \"\"\"\n",
        "    logging.info(f\"Scraping table data & metadata from: {url}\")\n",
        "    # Prepare headers for both initial HTML and potential AJAX request\n",
        "    headers = {'User-Agent': DEFAULT_USER_AGENT, 'Accept': 'application/json, text/javascript, */*; q=0.01', 'X-Requested-With': 'XMLHttpRequest'}\n",
        "    result: Dict[str, Any] = {'dataframes': [], 'filing_date': None, 'quarter': None}\n",
        "    html_content = None\n",
        "\n",
        "    # 1. Fetch Initial HTML Page\n",
        "    try:\n",
        "        response_page = requests.get(url, headers=headers, timeout=40) # Increased timeout\n",
        "        response_page.raise_for_status()\n",
        "        html_content = response_page.text\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Failed to retrieve page {url}. Status: {getattr(e.response, 'status_code', 'N/A')}. Error: {e}\")\n",
        "        return result # Cannot proceed without HTML\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Unexpected error during page fetch for {url}: {e}\")\n",
        "        return result\n",
        "\n",
        "    # 2. Parse HTML & Extract Metadata (if HTML fetched)\n",
        "    soup = None\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'lxml')\n",
        "        # Filing Date\n",
        "        date_dt = soup.find('dt', string=lambda t: t and 'Date filed' in t.strip())\n",
        "        if date_dt and date_dt.find_next_sibling('dd'):\n",
        "            result['filing_date'] = date_dt.find_next_sibling('dd').get_text(strip=True)\n",
        "        # Quarter Title (broader search)\n",
        "        qtr_h = soup.find(['h1', 'h2', 'h3'], string=lambda t: t and ('13F Holdings' in t or 'Quarter' in t))\n",
        "        if qtr_h: result['quarter'] = qtr_h.get_text(strip=True)\n",
        "        logging.info(f\"Metadata extracted - Date: {result['filing_date']}, Quarter: {result['quarter']}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error parsing HTML or extracting metadata from {url}: {e}\")\n",
        "        # Continue, attempt table parsing anyway\n",
        "\n",
        "    # 3. Find Target Table (usually the main data table)\n",
        "    target_table = soup.find('table', id='filingAggregated') if soup else None\n",
        "\n",
        "    if not target_table:\n",
        "        logging.warning(f\"Target table (id='filingAggregated') not found. Parsing all tables on page: {url}\")\n",
        "        if soup: result['dataframes'] = parse_tables_directly_bs4(soup, url) # Parse all tables as fallback\n",
        "        return result\n",
        "\n",
        "    # 4. Extract HTML Headers (Needed for both AJAX and direct parsing)\n",
        "    headers_list = _extract_headers(target_table)\n",
        "    if not headers_list:\n",
        "        logging.error(f\"CRITICAL: Failed to extract HTML headers from target table on {url}. Cannot reliably parse data.\")\n",
        "        # Try direct parsing anyway, it might find headers differently, but less reliable\n",
        "        result['dataframes'] = parse_tables_directly_bs4(target_table, url, single_table=True)\n",
        "        return result\n",
        "\n",
        "    # 5. Check for AJAX 'data-url'\n",
        "    data_url_path = target_table.get('data-url')\n",
        "    ajax_succeeded = False\n",
        "\n",
        "    if data_url_path:\n",
        "        ajax_url = urljoin(BASE_URL, data_url_path)\n",
        "        logging.info(f\"AJAX endpoint detected. Attempting fetch from: {ajax_url}\")\n",
        "        try:\n",
        "            response_ajax = requests.get(ajax_url, headers=headers, timeout=40)\n",
        "            response_ajax.raise_for_status() # Check for server errors (like 500)\n",
        "            ajax_data = response_ajax.json()\n",
        "            table_rows_data = ajax_data.get('data') # Standard key for DataTables\n",
        "\n",
        "            if table_rows_data and isinstance(table_rows_data, list):\n",
        "                expected_cols = len(headers_list)\n",
        "                cleaned_rows = []\n",
        "                mismatched_logged = False\n",
        "                for i, row in enumerate(table_rows_data):\n",
        "                    if isinstance(row, list):\n",
        "                        actual_cols = len(row)\n",
        "                        if actual_cols >= expected_cols:\n",
        "                            processed_row = row[:expected_cols] # Slice if too many cols\n",
        "                            if actual_cols > expected_cols and not mismatched_logged:\n",
        "                                logging.warning(f\"AJAX row {i} column count ({actual_cols}) > header count ({expected_cols}). Using first {expected_cols}.\")\n",
        "                                mismatched_logged = True\n",
        "                        else: # actual_cols < expected_cols\n",
        "                            if not mismatched_logged:\n",
        "                                logging.warning(f\"AJAX row {i} column count ({actual_cols}) < header count ({expected_cols}). Skipping row.\")\n",
        "                                mismatched_logged = True\n",
        "                            continue # Skip row\n",
        "\n",
        "                        # Clean HTML within cells\n",
        "                        cleaned_cells = [BeautifulSoup(str(c), 'lxml').get_text(strip=True) if isinstance(c, str) else c for c in processed_row]\n",
        "                        cleaned_rows.append(cleaned_cells)\n",
        "                    else:\n",
        "                        logging.warning(f\"Skipping AJAX row {i} due to non-list format: {type(row)}\")\n",
        "\n",
        "                if cleaned_rows:\n",
        "                    # Create DataFrame using HTML headers and cleaned AJAX data\n",
        "                    df = pd.DataFrame(cleaned_rows, columns=headers_list)\n",
        "                    df['SourceURL'] = url\n",
        "                    result['dataframes'].append(df)\n",
        "                    ajax_succeeded = True\n",
        "                    logging.info(f\"Successfully created DataFrame (Shape: {df.shape}) from AJAX data.\")\n",
        "                else:\n",
        "                    logging.warning(f\"Processed AJAX response but no valid rows constructed for {url}.\")\n",
        "            else:\n",
        "                logging.warning(f\"AJAX response missing 'data' key or 'data' is not a list for {ajax_url}.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            # Log specific HTTP error if available\n",
        "            http_status = getattr(e.response, 'status_code', 'N/A')\n",
        "            logging.error(f\"AJAX request failed for {ajax_url}. Status: {http_status}. Error: {e}\") # Log 500 error here\n",
        "        except json.JSONDecodeError as e:\n",
        "            logging.error(f\"Failed to decode JSON from {ajax_url}. Error: {e}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Unexpected error processing AJAX for {url}. Error: {e}\", exc_info=True)\n",
        "\n",
        "        # If AJAX path was taken and succeeded, return result now\n",
        "        if ajax_succeeded:\n",
        "            return result\n",
        "        else:\n",
        "            logging.warning(f\"AJAX processing did not yield a DataFrame for {url}.\") # Fall through to direct parse\n",
        "\n",
        "    # 6. Fallback to Direct Parsing (if no data-url OR if AJAX failed)\n",
        "    logging.info(f\"Falling back to direct HTML parsing for target table on: {url}\")\n",
        "    result['dataframes'] = parse_tables_directly_bs4(target_table, url, single_table=True)\n",
        "    return result\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                        Execution Stage 1: Get Manager Links\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 1: Scraping Manager Links\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "manager_links_relative: List[str] = scrape_manager_links(BASE_URL + \"/\")\n",
        "print(f\"Found {len(manager_links_relative)} unique relative manager links.\")\n",
        "\n",
        "if not manager_links_relative:\n",
        "    print(\"No manager links found. Exiting script.\")\n",
        "    exit()\n",
        "\n",
        "# Optional: Limit managers for testing\n",
        "# test_limit = 10\n",
        "# print(f\"\\n*** LIMITING MANAGERS TO {test_limit} FOR TESTING ***\\n\")\n",
        "# manager_links_relative = manager_links_relative[:test_limit]\n",
        "\n",
        "time.sleep(random.uniform(0.5, 1.0)) # Small delay before next stage\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                 Execution Stage 2: Get Quarter Links per Manager\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 2: Scraping Quarter Links for Each Manager\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "manager_quarter_links: Dict[str, List[str]] = {}\n",
        "num_managers = len(manager_links_relative)\n",
        "\n",
        "for i, rel_link in enumerate(manager_links_relative):\n",
        "    print(f\"\\n[{i+1}/{num_managers}] Getting quarter links for Manager: {rel_link}\")\n",
        "    links = scrape_quarter_links_from_manager(rel_link, BASE_URL)\n",
        "    manager_quarter_links[rel_link] = links # Store links (or empty list if failed)\n",
        "    if not links:\n",
        "        logging.warning(f\"No quarter links found or error occurred for {rel_link}\")\n",
        "    # Polite delay between manager page requests\n",
        "    time.sleep(random.uniform(1.0, 3.0))\n",
        "\n",
        "print(f\"\\nFinished getting quarter links for {len(manager_quarter_links)} managers processed.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                 Execution Stage 3: Scrape Table Data & Metadata\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 3: Scraping Table Data and Metadata for Each Filing\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "all_scraped_dataframes: List[pd.DataFrame] = [] # List to hold all successfully scraped DataFrames\n",
        "total_managers_to_scrape = len(manager_quarter_links)\n",
        "\n",
        "for i, (manager_link, quarter_links) in enumerate(manager_quarter_links.items()):\n",
        "    print(f\"\\n>>> Processing Manager {i+1}/{total_managers_to_scrape}: {manager_link} <<<\")\n",
        "    logging.info(f\"Processing Manager [{i+1}/{total_managers_to_scrape}]: {manager_link}\")\n",
        "\n",
        "    # Get Fund Name (once per manager)\n",
        "    manager_page_url = urljoin(BASE_URL, manager_link)\n",
        "    current_fund_name = scrape_fund_name(manager_page_url)\n",
        "    print(f\"    Fund Name: {current_fund_name}\")\n",
        "\n",
        "    # Determine URLs to scrape (quarter links or manager page if none)\n",
        "    urls_to_scrape = quarter_links if quarter_links else [manager_page_url]\n",
        "    if not quarter_links:\n",
        "        logging.warning(f\"No specific quarter links found for {manager_link}, attempting scrape on manager page directly: {manager_page_url}\")\n",
        "\n",
        "    total_urls_for_manager = len(urls_to_scrape)\n",
        "    print(f\"    Preparing to scrape data from {total_urls_for_manager} URL(s)...\")\n",
        "\n",
        "    # Scrape data for each URL associated with this manager\n",
        "    for j, data_url in enumerate(urls_to_scrape):\n",
        "        print(f\"      -> Scraping URL [{j+1}/{total_urls_for_manager}]: {data_url}\")\n",
        "        # Call the main scraping function\n",
        "        scrape_result = scrape_table_data_and_metadata(data_url)\n",
        "        scraped_dfs_list = scrape_result.get('dataframes', [])\n",
        "        filing_date = scrape_result.get('filing_date')\n",
        "        quarter_str = scrape_result.get('quarter')\n",
        "\n",
        "        if scraped_dfs_list:\n",
        "            logging.info(f\"Found {len(scraped_dfs_list)} table(s) on {data_url}.\")\n",
        "            print(f\"        -- Found {len(scraped_dfs_list)} table(s) from this URL --\")\n",
        "            # Process each DataFrame found on this page\n",
        "            for df_scraped in scraped_dfs_list:\n",
        "                # Add metadata collected for this URL\n",
        "                df_scraped['fund_name'] = current_fund_name\n",
        "                df_scraped['filing_date'] = filing_date\n",
        "                df_scraped['quarter'] = quarter_str\n",
        "                # Append the processed DataFrame to the master list\n",
        "                all_scraped_dataframes.append(df_scraped)\n",
        "        else:\n",
        "            logging.warning(f\"No data tables were successfully extracted from {data_url}.\")\n",
        "            print(f\"        -- No data tables extracted from this URL (check warnings/errors above) --\")\n",
        "\n",
        "        # Polite delay between *each data URL request* - IMPORTANT\n",
        "        time.sleep(random.uniform(1.8, 4.8)) # Slightly increased delay\n",
        "\n",
        "print(\"\\n--- Finished Scraping All Filing Data ---\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#               Execution Stage 4: Data Consolidation & Processing\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 4: Consolidating and Processing All Scraped Data\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not all_scraped_dataframes:\n",
        "    logging.warning(\"No dataframes were scraped in Stage 3. Cannot proceed with processing.\")\n",
        "    final_processed_df = pd.DataFrame() # Ensure variable exists but is empty\n",
        "else:\n",
        "    # --- Combine all scraped dataframes ---\n",
        "    logging.info(f\"Combining {len(all_scraped_dataframes)} scraped dataframes...\")\n",
        "    print(f\"Combining {len(all_scraped_dataframes)} scraped tables...\")\n",
        "    try:\n",
        "        combined_df = pd.concat(all_scraped_dataframes, ignore_index=True, sort=False)\n",
        "        logging.info(f\"Initial combined DataFrame shape: {combined_df.shape}\")\n",
        "        print(f\"Combined DataFrame shape: {combined_df.shape}.\")\n",
        "        # It's crucial to work on a copy for significant processing\n",
        "        processed_df = combined_df.copy()\n",
        "        del combined_df # Free up memory from the large intermediate list/df\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"CRITICAL Error during DataFrame concatenation: {e}\", exc_info=True)\n",
        "        processed_df = pd.DataFrame() # Assign empty df on critical error\n",
        "\n",
        "# --- Process the combined dataframe (only if concatenation was successful) ---\n",
        "if not processed_df.empty:\n",
        "    logging.info(\"Starting data processing steps...\")\n",
        "    print(\"\\nProcessing combined data...\")\n",
        "\n",
        "    # --- Standardize Column Names (apply cleaning to all columns) ---\n",
        "    processed_df.columns = [_clean_header(col) for col in processed_df.columns]\n",
        "    logging.info(f\"Standardized column names: {processed_df.columns.tolist()}\")\n",
        "\n",
        "    # --- Filter for Common Stock ('COM') ---\n",
        "    if 'cl' in processed_df.columns:\n",
        "        original_rows = len(processed_df)\n",
        "        # Ensure case-insensitivity and handle potential NaN\n",
        "        processed_df = processed_df[processed_df['cl'].astype(str).str.upper() == 'COM']\n",
        "        logging.info(f\"Filtered for 'cl' == 'COM'. Rows reduced from {original_rows} to {len(processed_df)}.\")\n",
        "        print(f\"Filtered for 'COM' stock. Rows remaining: {len(processed_df)}.\")\n",
        "    else:\n",
        "        logging.warning(\"'cl' column not found, skipping filter for common stock.\")\n",
        "\n",
        "    # --- Data Type Conversion and Cleaning ---\n",
        "    logging.info(\"Cleaning and converting data types...\")\n",
        "    # Shares (convert to numeric)\n",
        "    if 'shares' in processed_df.columns:\n",
        "        processed_df['shares'] = processed_df['shares'].astype(str).str.replace(',', '', regex=False)\n",
        "        processed_df['shares_numeric'] = pd.to_numeric(processed_df['shares'], errors='coerce')\n",
        "        nan_shares_count = processed_df['shares_numeric'].isna().sum()\n",
        "        if nan_shares_count > 0:\n",
        "            logging.warning(f\"{nan_shares_count} 'shares' values could not be converted to numeric (set to NaN).\")\n",
        "    else:\n",
        "        logging.warning(\"'shares' column not found. Creating 'shares_numeric' as NaN.\")\n",
        "        processed_df['shares_numeric'] = np.nan\n",
        "\n",
        "    # Value Column (convert to numeric)\n",
        "    value_col = 'value_usd_000'\n",
        "    if value_col in processed_df.columns:\n",
        "         processed_df[value_col] = processed_df[value_col].astype(str).str.replace(',', '', regex=False)\n",
        "         processed_df[value_col] = pd.to_numeric(processed_df[value_col], errors='coerce')\n",
        "         nan_value_count = processed_df[value_col].isna().sum()\n",
        "         if nan_value_count > 0:\n",
        "             logging.warning(f\"{nan_value_count} '{value_col}' values could not be converted to numeric (set to NaN).\")\n",
        "    else:\n",
        "         logging.warning(f\"Value column '{value_col}' not found.\")\n",
        "\n",
        "    # Quarter Period (create sortable representation)\n",
        "    if 'quarter' in processed_df.columns:\n",
        "        def parse_quarter_to_period(q_str: Optional[str]) -> Optional[pd.Period]:\n",
        "            \"\"\"Parses 'QX YYYY ...' string to pandas Period object.\"\"\"\n",
        "            if not isinstance(q_str, str): return pd.NaT\n",
        "            # More robust regex: Optional space, ignore case\n",
        "            match = re.search(r'(Q[1-4])\\s*(\\d{4})', q_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                q_num, year = match.group(1).upper(), match.group(2)\n",
        "                try: return pd.Period(f\"{year}{q_num}\", freq='Q')\n",
        "                except ValueError: return pd.NaT # Handle invalid year/quarter combo\n",
        "            return pd.NaT # Return Not-a-Time if pattern not found\n",
        "        processed_df['quarter_period'] = processed_df['quarter'].apply(parse_quarter_to_period)\n",
        "        nan_quarter_parse_count = processed_df['quarter_period'].isna().sum()\n",
        "        if nan_quarter_parse_count > 0:\n",
        "             logging.warning(f\"{nan_quarter_parse_count} 'quarter' values could not be parsed into sortable periods.\")\n",
        "    else:\n",
        "        logging.warning(\"'quarter' column not found. Cannot sort chronologically.\")\n",
        "        processed_df['quarter_period'] = pd.NaT # Create column anyway for safety\n",
        "\n",
        "    # --- Identify Stock Identifier ---\n",
        "    # Determine the primary column for stock identification ('sym' preferred)\n",
        "    if 'sym' in processed_df.columns:\n",
        "        stock_id_col = 'sym'\n",
        "    elif 'stock_symbol' in processed_df.columns:\n",
        "        stock_id_col = 'stock_symbol'\n",
        "    else:\n",
        "        stock_id_col = None\n",
        "    if stock_id_col:\n",
        "        logging.info(f\"Using '{stock_id_col}' as the primary stock identifier for grouping.\")\n",
        "    else:\n",
        "        logging.error(\"CRITICAL: Neither 'sym' nor 'stock_symbol' column found. Cannot perform grouped calculations.\")\n",
        "\n",
        "    # --- Calculate Changes (Grouped Operations) ---\n",
        "    # Proceed only if necessary columns are present and valid\n",
        "    can_calculate_changes = (\n",
        "        stock_id_col is not None and\n",
        "        'shares_numeric' in processed_df.columns and\n",
        "        'quarter_period' in processed_df.columns and\n",
        "        processed_df['quarter_period'].notna().any() # Check if at least some quarters are parseable\n",
        "    )\n",
        "\n",
        "    if can_calculate_changes:\n",
        "        logging.info(f\"Calculating changes grouped by 'fund_name', '{stock_id_col}'...\")\n",
        "        print(f\"Calculating quarterly changes grouped by fund and '{stock_id_col}'...\")\n",
        "\n",
        "        # IMPORTANT: Drop rows with NaNs in essential grouping/sorting keys to avoid errors\n",
        "        essential_cols = ['fund_name', stock_id_col, 'quarter_period', 'shares_numeric']\n",
        "        rows_before_drop = len(processed_df)\n",
        "        processed_df.dropna(subset=[col for col in essential_cols if col in processed_df.columns], inplace=True)\n",
        "        rows_after_drop = len(processed_df)\n",
        "        if rows_before_drop > rows_after_drop:\n",
        "            logging.warning(f\"Dropped {rows_before_drop - rows_after_drop} rows with NaN in essential columns ({essential_cols}) before change calculation.\")\n",
        "\n",
        "        # Perform sorting on the cleaned data\n",
        "        processed_df = processed_df.sort_values(\n",
        "            by=['fund_name', stock_id_col, 'quarter_period'],\n",
        "            ascending=True\n",
        "        )\n",
        "\n",
        "        # Define grouping columns\n",
        "        group_cols = ['fund_name', stock_id_col]\n",
        "\n",
        "        # Calculate 'change' (difference from previous quarter within group)\n",
        "        if 'change' not in processed_df.columns: processed_df['change'] = pd.NA # Ensure column exists\n",
        "        processed_df['change'] = processed_df.groupby(group_cols, observed=True)['shares_numeric'].diff().fillna(0)\n",
        "\n",
        "        # Calculate 'pct_change'\n",
        "        if 'pct_change' not in processed_df.columns: processed_df['pct_change'] = pd.NA # Ensure column exists\n",
        "        # Calculate pct_change, handle potential division by zero\n",
        "        pct_change_raw = processed_df.groupby(group_cols, observed=True)['shares_numeric'].pct_change()\n",
        "        # Replace inf/-inf with NaN, then fill remaining NaN (first entry of group) with 0\n",
        "        processed_df['pct_change'] = pct_change_raw.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "\n",
        "        # Infer 'inferred_transaction_type'\n",
        "        if 'inferred_transaction_type' not in processed_df.columns: processed_df['inferred_transaction_type'] = pd.NA # Ensure column exists\n",
        "        # Ensure 'change' is numeric before comparison\n",
        "        change_numeric = pd.to_numeric(processed_df['change'], errors='coerce')\n",
        "        conditions = [change_numeric < 0, change_numeric == 0, change_numeric > 0]\n",
        "        choices = ['Sell', 'Hold', 'Buy']\n",
        "        processed_df['inferred_transaction_type'] = np.select(conditions, choices, default='Unknown') # Default for NaN change\n",
        "\n",
        "        logging.info(\"Calculations for 'change', 'pct_change', 'inferred_transaction_type' complete.\")\n",
        "        print(\"Quarterly change calculations complete.\")\n",
        "\n",
        "        # Optional: Convert 'change' to nullable integer type for cleaner output\n",
        "        try: processed_df['change'] = processed_df['change'].astype(pd.Int64Dtype())\n",
        "        except TypeError: pass # Keep as float if conversion fails\n",
        "\n",
        "    else: # Cannot calculate changes\n",
        "        logging.warning(\"Skipping quarterly change calculations due to missing essential columns or unparseable quarters.\")\n",
        "        print(\"Skipping quarterly change calculations (missing required columns or data).\")\n",
        "        # Ensure placeholder columns exist even if calculations are skipped\n",
        "        for col in ['change', 'pct_change', 'inferred_transaction_type']:\n",
        "             if col not in processed_df.columns: processed_df[col] = pd.NA\n",
        "\n",
        "\n",
        "    # --- Final Column Formatting ---\n",
        "    logging.info(\"Formatting final columns...\")\n",
        "    print(\"\\nFormatting final columns for output...\")\n",
        "    # Rename stock identifier column to 'stock symbol' if needed\n",
        "    if stock_id_col == 'sym' and 'sym' in processed_df.columns:\n",
        "        processed_df.rename(columns={'sym': 'stock symbol'}, inplace=True)\n",
        "        logging.info(\"Renamed 'sym' column to 'stock symbol'.\")\n",
        "    elif stock_id_col == 'stock_symbol' and 'stock symbol' not in processed_df.columns and 'sym' in processed_df.columns:\n",
        "        logging.warning(\"Used 'stock_symbol' as ID, but column 'stock symbol' doesn't exist while 'sym' does. Check logic.\")\n",
        "    elif stock_id_col is None and 'sym' in processed_df.columns:\n",
        "        # If we couldn't group but 'sym' exists, still rename for consistency if target includes 'stock symbol'\n",
        "         if 'stock symbol' in TARGET_FINAL_COLUMNS:\n",
        "              processed_df.rename(columns={'sym': 'stock symbol'}, inplace=True)\n",
        "              logging.info(\"Renamed 'sym' to 'stock symbol' even though grouping failed.\")\n",
        "\n",
        "\n",
        "    # Ensure value column name is correct in target list if it was cleaned\n",
        "    if 'value_usd_000' not in TARGET_FINAL_COLUMNS and 'value_usd_000' in processed_df.columns:\n",
        "        # Adjust target list if needed, or assume 'value_usd_000' is intended\n",
        "        logging.warning(f\"Value column name used is 'value_usd_000', check consistency with TARGET_FINAL_COLUMNS: {TARGET_FINAL_COLUMNS}\")\n",
        "\n",
        "\n",
        "    # Select and order target columns, dropping others\n",
        "    final_columns_existing = [col for col in TARGET_FINAL_COLUMNS if col in processed_df.columns]\n",
        "    missing_target_cols = [col for col in TARGET_FINAL_COLUMNS if col not in final_columns_existing]\n",
        "    if missing_target_cols:\n",
        "        logging.warning(f\"Final target columns missing from processed data: {missing_target_cols}\")\n",
        "        print(f\"Warning: Final columns missing: {missing_target_cols}\")\n",
        "\n",
        "    final_processed_df = processed_df[final_columns_existing] # Select only existing target columns\n",
        "    logging.info(f\"Final selected columns: {final_processed_df.columns.tolist()}\")\n",
        "    print(f\"Final DataFrame columns: {final_processed_df.columns.tolist()}\")\n",
        "\n",
        "    # Drop temporary columns explicitly if they weren't dropped by selection\n",
        "    final_processed_df = final_processed_df.drop(columns=['shares_numeric', 'quarter_period'], errors='ignore')\n",
        "\n",
        "else: # combined_df was empty\n",
        "    logging.warning(\"Combined DataFrame was empty after scraping. No processing performed.\")\n",
        "    final_processed_df = pd.DataFrame() # Ensure variable exists but is empty\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "#                        Execution Stage 5: Final Output\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" Stage 5: Final Output\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "if not final_processed_df.empty:\n",
        "    # --- Print Head ---\n",
        "    print(f\"\\n--- First 5 rows of the final DataFrame (Shape: {final_processed_df.shape}) ---\")\n",
        "    pd.set_option('display.max_columns', None) # Show all columns for head\n",
        "    pd.set_option('display.width', 200)        # Adjust width for better viewing\n",
        "    pd.set_option('display.max_colwidth', 70)  # Limit column width slightly for readability\n",
        "    print(final_processed_df.head().to_string())\n",
        "    pd.reset_option('display.max_colwidth') # Reset column width\n",
        "\n",
        "    # --- Save to CSV ---\n",
        "    print(f\"\\n--- Saving final DataFrame to '{CSV_FILENAME}' ---\")\n",
        "    try:\n",
        "        final_processed_df.to_csv(CSV_FILENAME, index=False, encoding='utf-8-sig')\n",
        "        print(f\"Successfully saved data ({len(final_processed_df)} rows) to '{CSV_FILENAME}'\")\n",
        "    except Exception as e:\n",
        "        print(f\"CRITICAL ERROR: Failed to save the final DataFrame to CSV '{CSV_FILENAME}'. Error: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nFinal DataFrame is empty or was not created due to errors. No CSV file saved.\")\n",
        "\n",
        "print(\"\\n--- Script execution finished ---\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9JwCbLZ2yt6cX6b1xwYf3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}